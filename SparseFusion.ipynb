{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "7423d594-7c2c-4a5b-8089-3faf9e30b116",
          "showTitle": false,
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ozzn44gSRw5F",
        "outputId": "df1ead63-9e83-46e1-8143-a2b610c80999"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mlflow in /usr/local/lib/python3.11/dist-packages (3.1.1)\n",
            "Requirement already satisfied: mlflow-skinny==3.1.1 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.1.1)\n",
            "Requirement already satisfied: Flask<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.1.1)\n",
            "Requirement already satisfied: alembic!=1.10.0,<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.16.2)\n",
            "Requirement already satisfied: docker<8,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (7.1.0)\n",
            "Requirement already satisfied: graphene<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.4.3)\n",
            "Requirement already satisfied: gunicorn<24 in /usr/local/lib/python3.11/dist-packages (from mlflow) (23.0.0)\n",
            "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.0.2)\n",
            "Requirement already satisfied: pandas<3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.2.2)\n",
            "Requirement already satisfied: pyarrow<21,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (18.1.0)\n",
            "Requirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.6.1)\n",
            "Requirement already satisfied: scipy<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.15.3)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.0.41)\n",
            "Requirement already satisfied: cachetools<7,>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (8.2.1)\n",
            "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (3.1.1)\n",
            "Requirement already satisfied: databricks-sdk<1,>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (0.57.0)\n",
            "Requirement already satisfied: fastapi<1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (0.115.13)\n",
            "Requirement already satisfied: gitpython<4,>=3.1.9 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (3.1.44)\n",
            "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (8.7.0)\n",
            "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (1.34.1)\n",
            "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (1.34.1)\n",
            "Requirement already satisfied: packaging<26 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (24.2)\n",
            "Requirement already satisfied: protobuf<7,>=3.12.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3,>=1.10.8 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (2.11.7)\n",
            "Requirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (2.32.3)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (0.5.3)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (4.14.0)\n",
            "Requirement already satisfied: uvicorn<1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (0.34.3)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic!=1.10.0,<2->mlflow) (1.1.3)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from docker<8,>=4.0.0->mlflow) (2.4.0)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (1.9.0)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.1.6)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.0.2)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.1.3)\n",
            "Requirement already satisfied: graphql-core<3.3,>=3.1 in /usr/local/lib/python3.11/dist-packages (from graphene<4->mlflow) (3.2.6)\n",
            "Requirement already satisfied: graphql-relay<3.3,>=3.1 in /usr/local/lib/python3.11/dist-packages (from graphene<4->mlflow) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from graphene<4->mlflow) (2.9.0.post0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (3.2.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3->mlflow) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3->mlflow) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow) (3.6.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.2.3)\n",
            "Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.11/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow) (2.38.0)\n",
            "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi<1->mlflow-skinny==3.1.1->mlflow) (0.46.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython<4,>=3.1.9->mlflow-skinny==3.1.1->mlflow) (4.0.12)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==3.1.1->mlflow) (3.23.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.55b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.1.1->mlflow) (0.55b1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.1.1->mlflow) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.1.1->mlflow) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.1.1->mlflow) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.1.1->mlflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.1.1->mlflow) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.1.1->mlflow) (2025.6.15)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn<1->mlflow-skinny==3.1.1->mlflow) (0.16.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==3.1.1->mlflow) (5.0.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow) (4.9.1)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi<1->mlflow-skinny==3.1.1->mlflow) (4.9.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi<1->mlflow-skinny==3.1.1->mlflow) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow) (0.6.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install mlflow\n",
        "\n",
        "import base64\n",
        "import io\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.nn import init\n",
        "import mlflow\n",
        "\n",
        "from google.colab import files\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 667
        },
        "id": "8GT3JDCfshKX",
        "outputId": "1658de91-6747-4bdb-fe3e-b7394fbddd98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting sympy\n",
            "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy)\n",
            "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mpmath, sympy\n",
            "  Attempting uninstall: mpmath\n",
            "    Found existing installation: mpmath 1.3.0\n",
            "    Uninstalling mpmath-1.3.0:\n",
            "      Successfully uninstalled mpmath-1.3.0\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.13.1\n",
            "    Uninstalling sympy-1.13.1:\n",
            "      Successfully uninstalled sympy-1.13.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires sympy==1.13.1; python_version >= \"3.9\", but you have sympy 1.14.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed mpmath-1.3.0 sympy-1.14.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "673d7b570476423ebe58cacc361aaf32",
              "pip_warning": {
                "packages": [
                  "mpmath",
                  "sympy"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#!pip install --force-reinstall sympy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cgkfpc-0mGOk",
        "outputId": "60657647-d425-456a-9c87-da318039e1a4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/06/26 15:06:48 INFO mlflow.tracking.fluent: Experiment with name 'colab_experiment' does not exist. Creating a new experiment.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Experiment: artifact_location='file:///content/mlruns/225833999103561480', creation_time=1750950408813, experiment_id='225833999103561480', last_update_time=1750950408813, lifecycle_stage='active', name='colab_experiment', tags={}>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mlflow.set_tracking_uri(\"file:/content/mlruns\")\n",
        "mlflow.set_experiment(\"colab_experiment\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "71cb328e-20a2-42e7-95d2-ccdf9bcc238a",
          "showTitle": false,
          "title": ""
        },
        "id": "hcu64FfMRw5H"
      },
      "outputs": [],
      "source": [
        "# Ensure every computation happens on the GPU when available\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "3e279fd6-d26a-481e-b428-8629635da103",
          "showTitle": false,
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2LDm8CmRw5I",
        "outputId": "19e64902-ef90-4da3-8433-cc55db905b79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-06-26 15:38:28--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespear/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2025-06-26 15:38:28 ERROR 404: Not Found.\n",
            "\n",
            "Vocabulary size: 67\n",
            "This is a sample text for testing the model.\n",
            "Hello world!\n"
          ]
        }
      ],
      "source": [
        "#To build the encoding and decoding functions we use the tinyshakespear dataset. However for the sake of brevity we do not pretrain the decoder model on it\n",
        "#the training function should be able to do it without an issue as well as it could take both images and tex\n",
        "# Download the dataset\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespear/input.txt\n",
        "\n",
        "# Read the file\n",
        "text_path = \"input.txt\"\n",
        "with open(text_path, 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Add characters from the dataframe captions to the vocabulary\n",
        "caption_text = \" \".join(df['caption'].tolist())\n",
        "\n",
        "# Combine characters from input.txt and captions, and include all alphabet characters\n",
        "all_chars = sorted(list(set(text + caption_text + \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\")))\n",
        "\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(all_chars) }\n",
        "stoi['<pad>']= len(all_chars) # Assign a unique index for padding\n",
        "itos = { i:ch for i,ch in enumerate(all_chars) }\n",
        "itos[len(all_chars)] = '<pad>'\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "vocab_size = len(stoi.keys())\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "\n",
        "print(text[:500])  # preview first 500 characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "08daa24b-a3e9-46f9-bc57-51d8d1673397",
          "showTitle": false,
          "title": ""
        },
        "id": "TH449G_WRw5I"
      },
      "outputs": [],
      "source": [
        "class PatchEmbeddings(nn.Module):\n",
        "    def __init__(self, img_size=96, patch_size=16, hidden_dim=512):\n",
        "        super().__init__()\n",
        "\n",
        "        # Store the input image size\n",
        "        self.img_size = img_size\n",
        "\n",
        "        # Store the size of each patch\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "        # Calculate the total number of patches\n",
        "        self.num_patches = (img_size // patch_size) ** 2\n",
        "\n",
        "        # Create a convolutional layer to extract patch embeddings\n",
        "        # in_channels=3 assumes the input image has 3 color channels (RGB)\n",
        "        # out_channels=hidden_dim sets the number of output channels to match the hidden dimension\n",
        "        # kernel_size=patch_size and stride=patch_size ensure each patch is separately embedded\n",
        "        self.conv = nn.Conv2d(in_channels=3, out_channels=hidden_dim,\n",
        "                              kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, X):\n",
        "        # Extract patch embeddings from the input image\n",
        "        X = self.conv(X)\n",
        "\n",
        "        # Flatten the spatial dimensions (height and width) of the patch embeddings\n",
        "        # This step flattens the patch dimensions into a single dimension\n",
        "        X = X.flatten(2)\n",
        "\n",
        "        # Transpose the dimensions to obtain the shape [batch_size, num_patches, hidden_dim]\n",
        "        # This step brings the num_patches dimension to the second position\n",
        "        X = X.transpose(1, 2)\n",
        "\n",
        "        return X\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "5b3934cf-5735-470c-aece-7babbd097a38",
          "showTitle": false,
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhTi7MpFRw5I",
        "outputId": "bddd653d-3978-4040-8013-689cf1a6fc7a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([4, 36, 512])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#testing\n",
        "img_size, patch_size,  num_hiddens, batch_size = 96, 16, 512, 4\n",
        "patch_embeddings = PatchEmbeddings(img_size, patch_size, num_hiddens )\n",
        "X = torch.zeros(batch_size, 3, img_size, img_size)\n",
        "patch_embeddings(X).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "acec0b6d-4b53-497a-b0a4-8e8aee8270ee",
          "showTitle": false,
          "title": ""
        },
        "id": "_xJdDCXgRw5I"
      },
      "outputs": [],
      "source": [
        "#swapping linear for lazy linear for simplicity. Lazylinear can accept any arbitrary input dimension without having it specified\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, n_embd, dropout=0.1, is_decoder=True):\n",
        "        super().__init__()\n",
        "\n",
        "        # Define the layers of the MLP\n",
        "        layers = [\n",
        "            # First linear layer that expands the input dimension from n_embd to 4 * n_embd\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "\n",
        "            # Activation function: ReLU if is_decoder is True, else GELU\n",
        "            nn.ReLU() if is_decoder else nn.GELU(),\n",
        "\n",
        "            # Second linear layer that projects the intermediate dimension back to n_embd\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "\n",
        "            # Dropout layer for regularization\n",
        "            nn.Dropout(dropout)\n",
        "        ]\n",
        "\n",
        "        # Create a sequential container to hold the layers\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass the input through the MLP layers\n",
        "        return self.net(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "e56112ea-2647-45c2-b9ca-12f78a6827c7",
          "showTitle": false,
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ct1pFoFpRw5J",
        "outputId": "e4d6d419-908f-46df-e0d5-001d09d775af"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([4, 3, 128])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#For the sake of this example consider embedding size to be 128\n",
        "n_embd = 128\n",
        "testmlp = MLP(n_embd)\n",
        "mlp_input = torch.zeros(batch_size, 3, n_embd)\n",
        "testmlp_out = testmlp(mlp_input)\n",
        "testmlp_out.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "fff7d282-c2ab-4b90-ab4a-f99fa9cd14c8",
          "showTitle": false,
          "title": ""
        },
        "id": "uqj-YL7pRw5J"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "    def __init__(self, n_embd, head_size, dropout=0.1, is_decoder=False):\n",
        "        super().__init__()\n",
        "\n",
        "        # Linear layer for key projection\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "\n",
        "        # Linear layer for query projection\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "\n",
        "        # Linear layer for value projection\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "\n",
        "        # Dropout layer for regularization\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Flag indicating whether this head is used in the decoder\n",
        "        self.is_decoder = is_decoder\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Get the batch size (B), sequence length (T), and embedding dimension (C) from the input tensor\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        # Compute key, query, and value projections\n",
        "        k = self.key(x)   # Shape: [B, T, head_size]\n",
        "        q = self.query(x) # Shape: [B, T, head_size]\n",
        "        v = self.value(x) # Shape: [B, T, head_size]\n",
        "\n",
        "        # Compute attention scores by taking the dot product of query and key\n",
        "        # and scaling by the square root of the embedding dimension\n",
        "        wei = q @ k.transpose(-2, -1) * (C ** -0.5) # Shape: [B, T, T]\n",
        "\n",
        "        if self.is_decoder:\n",
        "            # If this head is used in the decoder, apply a causal mask to the attention scores\n",
        "            # to prevent attending to future positions\n",
        "            tril = torch.tril(torch.ones(T, T, dtype=torch.bool, device=x.device))\n",
        "            wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "\n",
        "        # Apply softmax to the attention scores to obtain attention probabilities\n",
        "        wei = F.softmax(wei, dim=-1) # Shape: [B, T, T]\n",
        "\n",
        "        # Apply dropout to the attention probabilities for regularization\n",
        "        wei = self.dropout(wei)\n",
        "\n",
        "        # Perform weighted aggregation of values using the attention probabilities\n",
        "        out = wei @ v # Shape: [B, T, head_size]\n",
        "\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "17f9b16f-0ebc-4b59-b285-78790613aa15",
          "showTitle": false,
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmD_ct_-Rw5J",
        "outputId": "a94a498d-ed3f-4130-d748-9104d1868982"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([4, 3, 16])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Example values for testing\n",
        "n_embd, head_size, batch_size = 128, 16, 4\n",
        "\n",
        "testhead = Head(n_embd, head_size)\n",
        "head_input = torch.zeros(batch_size, 3, n_embd)\n",
        "testhead_out = testhead(head_input)\n",
        "testhead_out.shape # (B, T,H_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "49de410a-f93b-4413-a06e-a7487379c5fc",
          "showTitle": false,
          "title": ""
        },
        "id": "-xHZhwnHRw5J"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, n_embd, num_heads, dropout=0.1, is_decoder=False):\n",
        "        super().__init__()\n",
        "\n",
        "        # Ensure that the embedding dimension is divisible by the number of heads\n",
        "        assert n_embd % num_heads == 0, \"n_embd must be divisible by num_heads\"\n",
        "\n",
        "        # Create a ModuleList of attention heads\n",
        "        self.heads = nn.ModuleList([\n",
        "            Head(n_embd, n_embd // num_heads, dropout, is_decoder)\n",
        "            for _ in range(num_heads)\n",
        "        ])\n",
        "\n",
        "        # Linear layer for projecting the concatenated head outputs\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "\n",
        "        # Dropout layer for regularization\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply each attention head to the input tensor\n",
        "        head_outputs = [h(x) for h in self.heads]\n",
        "\n",
        "        # Concatenate the outputs from all heads along the last dimension\n",
        "        out = torch.cat(head_outputs, dim=-1)\n",
        "\n",
        "        # Apply the projection layer to the concatenated outputs\n",
        "        out = self.proj(out)\n",
        "\n",
        "        # Apply dropout to the projected outputs for regularization\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "90c71fe2-3ef6-4eca-9805-130b12418646",
          "showTitle": false,
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qudiJJl7Rw5J",
        "outputId": "0f3ec5a1-9f25-4265-8046-c0d035600b58"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([4, 3, 128])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Example values for testing\n",
        "n_embd, n_head = 128, 8\n",
        "testmha = MultiHeadAttention(n_embd, n_head)\n",
        "head_input = torch.zeros(batch_size, 3, n_embd)\n",
        "testmha_out = testmha(head_input)\n",
        "testmha_out.shape # (B, T,H_size*n_heads = n_embed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "eb477c79-fd05-4e79-8149-27b62d2b1c7e",
          "showTitle": false,
          "title": ""
        },
        "id": "xmSrzvZYRw5K"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd, num_heads, dropout=0.1, is_decoder=False):\n",
        "        super().__init__()\n",
        "\n",
        "        # Layer normalization for the input to the attention layer\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "\n",
        "        # Multi-head attention module\n",
        "        self.attn = MultiHeadAttention(n_embd, num_heads, dropout, is_decoder)\n",
        "\n",
        "        # Layer normalization for the input to the FFN\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "        # Feed-forward neural network (FFN)\n",
        "        self.ffn = MLP(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        original_x = x  # Save the input for the residual connection\n",
        "\n",
        "        # Apply layer normalization to the input\n",
        "        x = self.ln1(x)\n",
        "\n",
        "        # Apply multi-head attention\n",
        "        attn_output = self.attn(x)\n",
        "\n",
        "        # Add the residual connection (original input) to the attention output\n",
        "        x = original_x + attn_output\n",
        "\n",
        "        # Apply layer normalization to the input to the FFN\n",
        "        x = self.ln2(x)\n",
        "\n",
        "        # Apply the FFN\n",
        "        ffn_output = self.ffn(x)\n",
        "\n",
        "        # Add the residual connection (input to FFN) to the FFN output\n",
        "        x = x + ffn_output\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "ed52337a-c5ff-4164-bb5d-0e450c3a02fd",
          "showTitle": false,
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBxryt6vRw5K",
        "outputId": "a153e151-0ad4-4970-dd04-245c815966b2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([4, 3, 128])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Example values for testing\n",
        "n_embd, head_size, batch_size = 128, 16, 4\n",
        "\n",
        "testblock = Block(n_embd, n_head)\n",
        "block_input = torch.zeros(batch_size, 3, n_embd)\n",
        "testblock_out = testblock(block_input)\n",
        "testblock_out.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "a1791a86-6103-4658-9609-e560fb9ccd16",
          "showTitle": false,
          "title": ""
        },
        "id": "fRs4ppwaia9z"
      },
      "source": [
        "Now all this can be be put together to implement a Vision Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "fd6bdd3c-3c0c-413b-8800-a036a7848492",
          "showTitle": false,
          "title": ""
        },
        "id": "bSBuAN1GRw5K"
      },
      "outputs": [],
      "source": [
        "class ViT(nn.Module):\n",
        "    def __init__(self, img_size, patch_size, num_hiddens, num_heads, num_blks, emb_dropout, blk_dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        # Patch embedding layer to convert the input image into patches\n",
        "        self.patch_embedding = PatchEmbeddings(img_size, patch_size, num_hiddens)\n",
        "\n",
        "        # Learnable classification token\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, num_hiddens))\n",
        "\n",
        "        # Calculate the number of patches\n",
        "        num_patches = (img_size // patch_size) ** 2\n",
        "\n",
        "        # Learnable position embedding\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, num_hiddens))\n",
        "\n",
        "        # Dropout layer for the embeddings\n",
        "        self.dropout = nn.Dropout(emb_dropout)\n",
        "\n",
        "        # Stack of transformer blocks\n",
        "        self.blocks = nn.ModuleList([Block(num_hiddens, num_heads, blk_dropout, is_decoder=False) for _ in range(num_blks)])\n",
        "\n",
        "        # Layer normalization for the final representation\n",
        "        self.layer_norm = nn.LayerNorm(num_hiddens)\n",
        "\n",
        "    def forward(self, X):\n",
        "        # Convert the input image into patch embeddings\n",
        "        x = self.patch_embedding(X)\n",
        "\n",
        "        # Expand the classification token to match the batch size\n",
        "        cls_tokens = self.cls_token.expand(x.shape[0], -1, -1)\n",
        "\n",
        "        # Concatenate the classification token with the patch embeddings\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "\n",
        "        # Add the position embedding to the patch embeddings\n",
        "        x += self.pos_embedding\n",
        "\n",
        "        # Apply dropout to the embeddings\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Pass the embeddings through the transformer blocks\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        # Apply layer normalization to the final representation\n",
        "        x = self.layer_norm(x[:, 0])\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "c4c3dfa0-6e1a-42db-94b4-ea19fcdfe7a1",
          "showTitle": false,
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmTp3JN7Rw5K",
        "outputId": "4219cd03-209e-48d0-d2b9-4c1213a0bf86"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([4, 512])"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#For purposes of testing\n",
        "img_size, patch_size, num_hiddens, n_head, num_blks, dropout = 96, 16, 512, 8, 3, 0.1\n",
        "\n",
        "testvit = ViT(img_size, patch_size, num_hiddens, n_head, num_blks, dropout, dropout)\n",
        "vit_input = torch.zeros(batch_size, 3, img_size, img_size)\n",
        "testvit_out = testvit(vit_input)\n",
        "testvit_out.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "c0733d1d-fb0f-4ab2-879c-9a2fd05a4dc2",
          "showTitle": false,
          "title": ""
        },
        "id": "R3z7SAf-Rw5K"
      },
      "outputs": [],
      "source": [
        "class MultiModalProjector(nn.Module):\n",
        "    def __init__(self, n_embd, image_embed_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        # Define the projection network\n",
        "        self.net = nn.Sequential(\n",
        "            # Linear layer to expand the image embedding dimension\n",
        "            nn.Linear(image_embed_dim, 4 * image_embed_dim),\n",
        "\n",
        "            # GELU activation function\n",
        "            nn.GELU(),\n",
        "\n",
        "            # Linear layer to project the expanded image embeddings to the text embedding dimension\n",
        "            nn.Linear(4 * image_embed_dim, n_embd),\n",
        "\n",
        "            # Dropout layer for regularization\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass the input through the projection network\n",
        "        x = self.net(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "23de9c94-8ca3-4367-b91e-6179bd05b00d",
          "showTitle": false,
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCULLV9pRw5K",
        "outputId": "4fc9fae5-c17f-4042-e251-0c33f61da0ae"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([4, 128])"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Example values for testing\n",
        "n_embd,num_hiddens = 128, 512\n",
        "\n",
        "testmmp = MultiModalProjector(n_embd,num_hiddens)\n",
        "mmp_input = testvit_out\n",
        "testmmp_out = testmmp(mmp_input)\n",
        "testmmp_out.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "a79587c4-20b2-4418-bc4d-71b5234432ee",
          "showTitle": false,
          "title": ""
        },
        "id": "EJtc3ekvkDkr"
      },
      "outputs": [],
      "source": [
        "#Expert module\n",
        "class Expert(nn.Module):\n",
        "    \"\"\" An MLP is a simple linear layer followed by a non-linearity i.e. each Expert \"\"\"\n",
        "\n",
        "    def __init__(self, n_embed):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embed, 4 * n_embed),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embed, n_embed),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "d79faa17-f58b-4ca7-90e0-f3f2b9e718dc",
          "showTitle": false,
          "title": ""
        },
        "id": "iCQEY0UvkDkr"
      },
      "outputs": [],
      "source": [
        "\n",
        "#noisy top-k gating\n",
        "class NoisyTopkRouter(nn.Module):\n",
        "    def __init__(self, n_embed, num_experts, top_k):\n",
        "        super(NoisyTopkRouter, self).__init__()\n",
        "        self.top_k = top_k\n",
        "        #layer for router logits\n",
        "        self.topkroute_linear = nn.Linear(n_embed, num_experts)\n",
        "        self.noise_linear =nn.Linear(n_embed, num_experts)\n",
        "\n",
        "\n",
        "    def forward(self, mh_output):\n",
        "        # mh_ouput is the output tensor from multihead self attention block\n",
        "        logits = self.topkroute_linear(mh_output)\n",
        "\n",
        "        #Noise logits\n",
        "        noise_logits = self.noise_linear(mh_output)\n",
        "\n",
        "        #Adding scaled unit gaussian noise to the logits\n",
        "        noise = torch.randn_like(logits)*F.softplus(noise_logits)\n",
        "        noisy_logits = logits + noise\n",
        "\n",
        "        top_k_logits, indices = noisy_logits.topk(self.top_k, dim=-1)\n",
        "        zeros = torch.full_like(noisy_logits, float('-inf'))\n",
        "        sparse_logits = zeros.scatter(-1, indices, top_k_logits)\n",
        "        router_output = F.softmax(sparse_logits, dim=-1)\n",
        "        return router_output, indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "0bf702f0-63b5-4f31-a65a-d55e286eaeab",
          "showTitle": false,
          "title": ""
        },
        "id": "H2QUE16DkDks"
      },
      "outputs": [],
      "source": [
        "#Now create the sparse mixture of experts module\n",
        "class SparseMoE(nn.Module):\n",
        "    def __init__(self, n_embed, num_experts, top_k):\n",
        "        super(SparseMoE, self).__init__()\n",
        "        self.router = NoisyTopkRouter(n_embed, num_experts, top_k)\n",
        "        self.experts = nn.ModuleList([Expert(n_embed) for _ in range(num_experts)])\n",
        "        self.top_k = top_k\n",
        "\n",
        "    def forward(self, x):\n",
        "        gating_output, indices = self.router(x)\n",
        "        final_output = torch.zeros_like(x)\n",
        "\n",
        "        # Reshape inputs for batch processing\n",
        "        flat_x = x.view(-1, x.size(-1))\n",
        "        flat_gating_output = gating_output.view(-1, gating_output.size(-1))\n",
        "\n",
        "        # Process each expert in parallel\n",
        "        for i, expert in enumerate(self.experts):\n",
        "            # Create a mask for the inputs where the current expert is in top-k\n",
        "            expert_mask = (indices == i).any(dim=-1)\n",
        "            flat_mask = expert_mask.view(-1)\n",
        "\n",
        "            if flat_mask.any():\n",
        "                expert_input = flat_x[flat_mask]\n",
        "                expert_output = expert(expert_input)\n",
        "\n",
        "                # Extract and apply gating scores\n",
        "                gating_scores = flat_gating_output[flat_mask, i].unsqueeze(1)\n",
        "                weighted_output = expert_output * gating_scores\n",
        "\n",
        "                # Update final output additively by indexing and adding\n",
        "                final_output[expert_mask] += weighted_output.squeeze(1)\n",
        "\n",
        "        return final_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "bd4f734f-06ce-41d6-93a4-c01c0ebbe58b",
          "showTitle": false,
          "title": ""
        },
        "id": "1OXdTPvpkDks"
      },
      "outputs": [],
      "source": [
        "class SparseMoEBlock(nn.Module):\n",
        "    def __init__(self, n_embd, num_heads, num_experts, top_k, dropout=0.1, is_decoder=False):\n",
        "        super().__init__()\n",
        "\n",
        "        # Layer normalization for the input to the attention layer\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "\n",
        "        # Multi-head attention module\n",
        "        self.attn = MultiHeadAttention(n_embd, num_heads, dropout, is_decoder)\n",
        "\n",
        "        # Layer normalization for the input to the FFN\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "        # Feed-forward neural network (FFN)\n",
        "        self.sparseMoE = SparseMoE(n_embd, num_experts, top_k)\n",
        "\n",
        "    def forward(self, x):\n",
        "        original_x = x  # Save the input for the residual connection\n",
        "\n",
        "        # Apply layer normalization to the input\n",
        "        x = self.ln1(x)\n",
        "\n",
        "        # Apply multi-head attention\n",
        "        attn_output = self.attn(x)\n",
        "\n",
        "        # Add the residual connection (original input) to the attention output\n",
        "        x = original_x + attn_output\n",
        "\n",
        "        # Apply layer normalization to the input to the FFN\n",
        "        x = self.ln2(x)\n",
        "\n",
        "        # Apply the FFN\n",
        "        sparseMoE_output = self.sparseMoE(x)\n",
        "\n",
        "        # Add the residual connection (input to FFN) to the FFN output\n",
        "        x = x + sparseMoE_output\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "db59fdc2-a922-4530-92f2-f51d801b3f39",
          "showTitle": false,
          "title": ""
        },
        "id": "aL9FkWjxRw5K"
      },
      "outputs": [],
      "source": [
        "class MoEDecoderLanguageModel(nn.Module):\n",
        "    def __init__(self, n_embd, image_embed_dim, vocab_size, num_heads, n_layer, num_experts, top_k, use_images=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.use_images = use_images\n",
        "\n",
        "        # Token embedding table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "\n",
        "        # Position embedding table\n",
        "        self.position_embedding_table = nn.Embedding(1000, n_embd)\n",
        "\n",
        "        if use_images:\n",
        "            # Image projection layer to align image embeddings with text embeddings\n",
        "            self.image_projection = MultiModalProjector(n_embd, image_embed_dim)\n",
        "\n",
        "        # Stack of transformer decoder blocks\n",
        "        self.sparseMoEBlocks = nn.Sequential(*[SparseMoEBlock(n_embd, num_heads, num_experts, top_k, is_decoder=True) for _ in range(n_layer)])\n",
        "\n",
        "        # Final layer normalization\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "\n",
        "        # Language modeling head\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, image_embeds=None, targets=None):\n",
        "        # Get token embeddings from the input indices\n",
        "        tok_emb = self.token_embedding_table(idx)\n",
        "\n",
        "        if self.use_images and image_embeds is not None:\n",
        "            # Project and concatenate image embeddings with token embeddings\n",
        "            img_emb = self.image_projection(image_embeds).unsqueeze(1)\n",
        "            tok_emb = torch.cat([img_emb, tok_emb], dim=1)\n",
        "\n",
        "        # Get position embeddings\n",
        "        pos_emb = self.position_embedding_table(torch.arange(tok_emb.size(1), device=device)).unsqueeze(0)\n",
        "\n",
        "        # Add position embeddings to token embeddings\n",
        "        x = tok_emb + pos_emb\n",
        "\n",
        "        # Pass through the transformer decoder blocks\n",
        "        x = self.sparseMoEBlocks(x)\n",
        "\n",
        "        # Apply final layer normalization\n",
        "        x = self.ln_f(x)\n",
        "\n",
        "        # Get the logits from the language modeling head\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            if self.use_images and image_embeds is not None:\n",
        "                # Prepare targets by concatenating a dummy target for the image embedding\n",
        "                batch_size = idx.size(0)\n",
        "                targets = torch.cat([torch.full((batch_size, 1), -100, dtype=torch.long, device=device), targets], dim=1)\n",
        "\n",
        "            # Compute the cross-entropy loss\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-100)\n",
        "            return logits, loss\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def generate(self, idx, image_embeds, max_new_tokens):\n",
        "        # Get the batch size and sequence length\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # Initialize the generated sequence with the input indices\n",
        "        generated = idx\n",
        "\n",
        "        if self.use_images and image_embeds is not None:\n",
        "            # Project and concatenate image embeddings with token embeddings\n",
        "            img_emb = self.image_projection(image_embeds).unsqueeze(1)\n",
        "            current_output = torch.cat([img_emb, self.token_embedding_table(idx)], dim=1)\n",
        "        else:\n",
        "            current_output = self.token_embedding_table(idx)\n",
        "\n",
        "        # Generate new tokens iteratively\n",
        "        for i in range(max_new_tokens):\n",
        "            # Get the current sequence length\n",
        "            T_current = current_output.size(1)\n",
        "\n",
        "            # Get position embeddings for the current sequence length\n",
        "            current_pos_emb = self.position_embedding_table(torch.arange(T_current, device=device)).unsqueeze(0)\n",
        "\n",
        "            # Add position embeddings to the current output\n",
        "            current_output += current_pos_emb\n",
        "\n",
        "            # Pass through the transformer decoder blocks\n",
        "            for block in self.sparseMoEBlocks:\n",
        "                current_output = block(current_output)\n",
        "\n",
        "            # Get the logits for the last token\n",
        "            logits = self.lm_head(current_output[:, -1, :])\n",
        "\n",
        "            # Apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "            # Sample the next token based on the probabilities\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            # Concatenate the generated token to the generated sequence\n",
        "            generated = torch.cat((generated, idx_next), dim=1)\n",
        "\n",
        "            # Get the embeddings for the generated token\n",
        "            idx_next_emb = self.token_embedding_table(idx_next)\n",
        "\n",
        "            # Concatenate the generated token embeddings to the current output\n",
        "            current_output = torch.cat((current_output, idx_next_emb), dim=1)\n",
        "\n",
        "        return generated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "b58b7a9f-34fc-45c6-8bb0-31e89a989b07",
          "showTitle": false,
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDQO0JAvRw5K",
        "outputId": "3572c738-e081-462f-aec9-ad9abfc85e7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logits shape: torch.Size([10, 51, 1000]), Loss: 7.036016941070557\n",
            "Generated sequence shape: torch.Size([10, 70])\n"
          ]
        }
      ],
      "source": [
        "# I use n_layer to represent number of decoder transformer blocks and n_blks for the vision encoder to avoid confusion\n",
        "model = MoEDecoderLanguageModel(n_embd=128, image_embed_dim=256, vocab_size=1000, num_heads=8, n_layer=6,num_experts=8, top_k=2, use_images=True)\n",
        "model.to(device)\n",
        "# Dummy input\n",
        "B, T = 10, 50\n",
        "idx = torch.randint(0, 1000, (B, T)).to(device)\n",
        "image_embeds = torch.randn(B, 256).to(device)  # Assume image_embed_dim is 256\n",
        "\n",
        "targets = torch.randint(0, vocab_size, (B, T)).to(device)  # Only if you want to compute loss\n",
        "\n",
        "# Test forward pass\n",
        "# Check if you need to calculate loss by providing targets\n",
        "if targets is not None:\n",
        "    logits, loss = model(idx, image_embeds, targets)\n",
        "    print(f\"Logits shape: {logits.shape}, Loss: {loss}\")\n",
        "else:\n",
        "    logits = model(idx, image_embeds)  # Call without targets\n",
        "    print(f\"Logits shape: {logits.shape}\")\n",
        "\n",
        "# Test generation\n",
        "generated = model.generate(idx, image_embeds, max_new_tokens=20)\n",
        "print(f\"Generated sequence shape: {generated.shape}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "6702a660-0512-4152-8d56-97f0ebd562e8",
          "showTitle": false,
          "title": ""
        },
        "id": "FjklXZrPRw5K"
      },
      "outputs": [],
      "source": [
        "class VisionMoELanguageModel(nn.Module):\n",
        "    def __init__(self, n_embd, image_embed_dim, vocab_size, n_layer, img_size, patch_size, num_heads, num_blks, emb_dropout, blk_dropout, num_experts, top_k):\n",
        "        super().__init__()\n",
        "\n",
        "        # Set num_hiddens equal to image_embed_dim\n",
        "        num_hiddens = image_embed_dim\n",
        "\n",
        "        # Assert that num_hiddens is divisible by num_heads\n",
        "        assert num_hiddens % num_heads == 0, \"num_hiddens must be divisible by num_heads\"\n",
        "\n",
        "        # Initialize the vision encoder (ViT)\n",
        "        self.vision_encoder = ViT(img_size, patch_size, num_hiddens, num_heads, num_blks, emb_dropout, blk_dropout)\n",
        "\n",
        "        # Initialize the language model decoder (DecoderLanguageModel)\n",
        "        self.decoder = MoEDecoderLanguageModel(n_embd, image_embed_dim, vocab_size, num_heads, n_layer,num_experts, top_k, use_images=True)\n",
        "\n",
        "    def forward(self, img_array, idx, targets=None):\n",
        "        # Get the image embeddings from the vision encoder\n",
        "        image_embeds = self.vision_encoder(img_array)\n",
        "\n",
        "        # Check if the image embeddings are valid\n",
        "        if image_embeds.nelement() == 0 or image_embeds.shape[1] == 0:\n",
        "            raise ValueError(\"Something is wrong with the ViT model. It's returning an empty tensor or the embedding dimension is empty.\")\n",
        "\n",
        "        if targets is not None:\n",
        "            # If targets are provided, compute the logits and loss\n",
        "            logits, loss = self.decoder(idx, image_embeds, targets)\n",
        "            return logits, loss\n",
        "        else:\n",
        "            # If targets are not provided, compute only the logits\n",
        "            logits = self.decoder(idx, image_embeds)\n",
        "            return logits\n",
        "\n",
        "    def generate(self, img_array, idx, max_new_tokens):\n",
        "        # Get the image embeddings from the vision encoder\n",
        "        image_embeds = self.vision_encoder(img_array)\n",
        "\n",
        "        # Check if the image embeddings are valid\n",
        "        if image_embeds.nelement() == 0 or image_embeds.shape[1] == 0:\n",
        "            raise ValueError(\"Something is wrong with the ViT model. It's returning an empty tensor or the embedding dimension is empty.\")\n",
        "\n",
        "        # Generate new tokens using the language model decoder\n",
        "        generated_tokens = self.decoder.generate(idx, image_embeds, max_new_tokens)\n",
        "        return generated_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "3595ca42-b3b5-4733-bd3b-0dedd59a9ee5",
          "showTitle": false,
          "title": ""
        },
        "id": "H9nFWES0Rw5L"
      },
      "outputs": [],
      "source": [
        "image_embed_dim = num_hiddens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "2dd9be94-6ba3-4387-9a3a-139907173b4d",
          "showTitle": false,
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-iwMzhuRw5L",
        "outputId": "8a42528a-b827-4d37-a5bb-9bf810c857ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output from initialization forward pass: tensor([[[ 0.0237,  0.0390,  0.8044,  0.2121,  0.1992,  0.3345, -0.3111,\n",
            "          -0.2680, -0.0794,  0.1918, -0.1886, -0.0617,  0.3960,  0.5573,\n",
            "           0.2326,  0.4493,  0.1801, -0.6960,  0.9013, -0.8631,  0.5878,\n",
            "          -0.0425, -0.1558, -0.2279],\n",
            "         [-0.6853, -0.2185, -0.5364,  0.0957, -0.2091,  0.3283, -0.1334,\n",
            "          -1.0542, -0.7298,  0.6488,  0.4061, -0.3395,  0.7128,  0.6432,\n",
            "           0.1091, -0.6694, -0.1910, -0.0026,  0.6167, -0.5091,  0.0029,\n",
            "          -0.4518, -0.4555, -0.4607],\n",
            "         [-0.4834, -0.0142,  0.2442, -1.2916, -0.4780,  0.4710,  0.7730,\n",
            "           0.6770, -0.2997, -0.4705, -0.1810,  0.1211,  0.0432, -0.2521,\n",
            "           0.9981, -0.6347,  0.7857, -0.7204,  0.1409,  0.1217,  0.0156,\n",
            "           0.1070,  0.5041,  0.1062],\n",
            "         [-0.0695, -0.5151,  0.0347, -0.7086, -0.3700, -0.6737,  0.5359,\n",
            "          -0.0610,  0.1948, -0.4395, -0.0641,  0.6581, -0.4340,  0.6925,\n",
            "          -0.1075,  0.0457,  0.2997, -0.4076, -0.1102, -0.0336,  0.3111,\n",
            "           0.8105,  0.2634, -0.2902],\n",
            "         [-0.0136, -0.0915, -0.7402, -0.6283,  0.5694, -0.0074,  0.3636,\n",
            "          -0.7130, -0.9316, -0.1604,  0.2431,  0.4356, -1.1159,  0.6613,\n",
            "           0.2266, -0.0540,  0.5188, -0.1411, -0.4960, -0.8000,  0.2769,\n",
            "           0.0497, -0.5700, -0.7590],\n",
            "         [-0.0211,  0.1068, -0.8665, -0.6262,  0.1451,  0.3498,  0.4600,\n",
            "           0.9357, -0.8119, -0.3420,  0.6493, -0.3985, -1.0013,  0.5691,\n",
            "          -0.2933,  0.0363,  0.1489,  0.1208, -0.3744, -0.6110,  0.3053,\n",
            "          -0.8408,  0.0919, -0.3297],\n",
            "         [ 0.1481, -0.0923, -1.0662, -0.2832, -0.9231, -0.3558, -0.3687,\n",
            "          -0.2938, -0.9648,  0.2995,  0.6510,  0.4672, -0.4101,  0.3522,\n",
            "           0.2377, -0.4836,  0.0944,  0.3872, -0.1098, -0.4601,  0.3556,\n",
            "          -0.4875, -1.0600, -0.5378],\n",
            "         [ 0.1494, -0.6142,  0.6007,  0.0918, -0.3840, -1.1170,  0.7769,\n",
            "          -0.5237, -0.0199,  0.8046,  1.0021, -0.0606,  0.1599,  0.6730,\n",
            "          -1.0693,  0.4118, -0.1532,  0.2456,  0.5346, -0.4562, -0.0081,\n",
            "           0.9565, -0.7632,  0.4123],\n",
            "         [ 0.6183, -0.9315, -0.0410,  0.0762,  0.3699,  0.1087,  0.6092,\n",
            "           1.0704, -0.6196,  0.2948,  1.4756, -0.9356, -0.5168,  0.4585,\n",
            "          -0.3314, -0.8711,  0.5445, -0.6157, -1.1568, -0.1630,  0.9782,\n",
            "          -0.2002,  1.0831,  0.2484],\n",
            "         [-0.2759,  0.7594,  0.4110,  0.6041,  0.4347, -0.1224, -0.1202,\n",
            "           0.5722,  0.7186,  0.7092,  0.6341,  0.0115,  0.1384,  0.5084,\n",
            "          -0.1247, -0.7766, -0.2896,  0.8872, -0.7445, -0.4232, -0.1978,\n",
            "           0.0163, -0.1461,  0.1887],\n",
            "         [ 0.1730,  0.6220, -0.1588,  0.1265,  0.1349,  0.1463, -0.2634,\n",
            "           0.3418, -0.4328, -0.0955,  0.8708,  0.4367,  0.5142,  0.6332,\n",
            "          -0.1682,  0.4762, -0.2193, -0.1598, -0.6790, -0.0746, -0.1135,\n",
            "          -1.2061,  0.1140, -0.5610],\n",
            "         [-0.5676,  0.2967,  0.3804, -0.5107, -0.4949, -0.8804, -0.6051,\n",
            "          -0.4210, -0.4143,  0.5922,  0.3616, -0.1897,  0.3138, -0.4577,\n",
            "           0.4809, -0.7499, -0.9037, -0.4639,  0.0908, -0.5937, -0.2390,\n",
            "          -0.3463,  0.4255, -0.4031],\n",
            "         [ 0.1951, -0.2105, -0.1829,  0.3274,  0.8652, -0.2544, -0.2900,\n",
            "           0.3483, -0.2942,  0.9524,  0.8355, -0.0466, -0.5177,  0.8904,\n",
            "          -0.4845, -0.0219,  0.2989, -0.6480, -0.4717, -0.6494,  0.1249,\n",
            "           0.5160,  0.8179,  0.3692],\n",
            "         [-0.5777, -0.0650,  0.0049,  0.1734, -0.3167, -0.5591,  1.1442,\n",
            "          -0.5127, -0.3552, -0.5360, -0.5995, -0.1023,  0.1470,  0.1796,\n",
            "           0.7044, -0.8000,  0.8081, -0.4536, -0.0987, -0.0837,  0.6743,\n",
            "           0.0082, -0.1215, -0.4966],\n",
            "         [-0.2832, -0.1843,  1.6907,  0.6546,  0.9909, -1.1651,  0.1271,\n",
            "          -0.6628, -0.0907,  0.0599,  0.0765,  1.3565,  0.2803,  0.8000,\n",
            "          -0.5107,  0.1845, -0.1830, -0.2561,  0.0753, -0.8895,  0.1012,\n",
            "          -0.2978, -1.0017,  0.5560],\n",
            "         [-0.2371,  0.2938,  0.4479,  0.0171, -0.2882, -0.5947, -0.6275,\n",
            "          -0.4078,  0.5696,  0.0680,  1.4236,  0.0848,  0.6308, -0.2461,\n",
            "          -1.2539,  0.1851, -0.4743, -0.7682, -0.6196,  0.1232, -0.1156,\n",
            "           0.0438,  0.4543, -0.1495],\n",
            "         [-0.3055, -0.5206,  0.1477, -1.2279, -0.1914, -1.5979, -0.1835,\n",
            "           0.3824, -0.2872, -0.1722,  0.2756,  0.8247, -1.1033,  1.0917,\n",
            "           0.3312, -0.0511, -0.2039, -0.1376, -0.1515,  0.0842,  0.0857,\n",
            "           0.1526,  0.5876,  0.2000],\n",
            "         [ 0.4529,  0.3733,  0.2173,  0.1138,  0.2870,  0.1624,  0.4315,\n",
            "           0.7106, -0.3370,  0.0953, -0.3107, -0.3599,  0.1022, -0.0504,\n",
            "          -0.0080, -0.0057,  0.1496,  0.3189,  0.1486, -0.4832,  0.3109,\n",
            "          -0.0660, -0.2063,  0.2401],\n",
            "         [ 1.0330,  0.0076,  1.3622, -0.1264, -0.2420, -0.9827,  0.3367,\n",
            "           0.0872, -0.1450, -0.2507,  0.0933,  0.9682,  0.1716,  1.3844,\n",
            "           0.0681, -0.1058,  0.5827,  0.2527, -0.5772, -0.2396,  1.0442,\n",
            "          -0.0380,  1.0011,  0.7900],\n",
            "         [-1.3469,  0.4665,  0.4999,  0.1503, -0.4251, -0.8808, -0.1185,\n",
            "          -0.0899, -0.6176,  0.5082,  0.4984, -0.0259,  0.1270,  0.2843,\n",
            "           0.2564, -1.0524, -0.2458, -0.5410, -0.3145, -0.5233,  0.2534,\n",
            "          -0.6429, -0.4234,  0.1592],\n",
            "         [-0.7210,  0.4072, -0.6663, -0.5876, -0.5100, -0.6902, -0.1691,\n",
            "          -1.0096, -0.1024,  0.2063, -0.4140,  0.4596,  0.2960,  0.7758,\n",
            "           0.4864, -0.5191, -0.4428, -0.3247, -0.3147,  0.0081,  0.1805,\n",
            "          -0.7274, -0.1909, -0.0980],\n",
            "         [ 0.2433, -0.3168, -0.1338, -0.4555, -0.3405, -0.2954, -0.9299,\n",
            "          -0.2312, -0.6167, -0.1311,  0.6945,  0.2186, -0.3132,  1.1175,\n",
            "           0.2749, -0.6227,  0.8979, -1.1543, -0.5943, -0.0076,  0.3974,\n",
            "           0.0392,  0.4667,  0.1068],\n",
            "         [ 0.0475, -0.5860,  0.5077, -0.3317, -0.6862, -0.3877,  1.1804,\n",
            "           0.8764,  0.3328, -0.3461, -0.2024,  0.0382,  0.4753,  0.1973,\n",
            "          -0.1696, -0.3484, -0.1716,  0.2519, -0.3958, -0.5143,  1.0745,\n",
            "           0.0295,  0.8786, -0.6543],\n",
            "         [-0.0684,  0.0398,  0.0074, -0.7586,  0.1399, -0.3815,  0.1348,\n",
            "          -0.2873,  0.1742, -0.6566,  0.3508, -0.1463, -0.6667,  0.3773,\n",
            "          -0.5956,  0.3573,  0.7396, -0.6965, -0.1327,  0.3115, -0.4388,\n",
            "          -0.0145,  0.0927,  0.0785],\n",
            "         [-0.2564,  0.3327,  0.8462,  0.3743, -0.1106,  0.0234,  0.1712,\n",
            "           0.4007, -1.0023,  1.1460,  0.0188,  0.5528,  0.4387,  0.7377,\n",
            "           0.0023,  0.2668, -0.3370, -0.3444, -0.0283, -0.7843,  0.1993,\n",
            "          -1.0716, -0.2486, -0.3256],\n",
            "         [-0.8638,  0.5963,  1.2187, -0.1277, -0.0925, -0.6547, -0.7086,\n",
            "          -0.2129, -0.5840,  0.8776,  0.0477,  0.4812, -0.2082,  0.1519,\n",
            "           0.4721, -0.3685, -1.1866, -0.6280,  0.6333, -1.5456,  0.6966,\n",
            "          -0.0204, -1.3147, -0.1072],\n",
            "         [ 0.7557, -0.2086,  1.2543, -0.4639,  1.2935, -0.1204,  0.0066,\n",
            "          -1.9996, -0.6677,  0.8019,  0.3594, -0.5867, -1.4532,  0.2497,\n",
            "           0.2413, -0.6661, -0.2429,  0.1436, -0.1091,  0.3389,  0.4471,\n",
            "          -0.3298,  0.7145,  0.0300],\n",
            "         [-0.2886, -0.0295, -0.1230, -0.0283, -0.0404, -0.7088, -0.3314,\n",
            "           0.2639, -0.5648, -0.0823, -0.6437, -0.1413, -0.2904, -0.5193,\n",
            "           0.0795, -1.1687, -1.0523,  1.1982, -0.0574, -0.0111, -0.0842,\n",
            "          -0.5927, -1.4287, -0.5503],\n",
            "         [ 1.4174,  0.0872,  0.1335,  0.1125,  0.9959, -0.8849,  0.7617,\n",
            "           0.2079, -0.7747, -0.2749,  0.7977, -1.2793, -0.1950,  0.8982,\n",
            "          -0.4023, -1.0321,  0.0783,  0.3584, -0.7904, -0.3196,  0.0722,\n",
            "           0.1204,  0.4724,  0.2796],\n",
            "         [-0.0321, -0.1788, -0.8559, -0.2016, -0.2858, -1.1124, -0.8393,\n",
            "          -0.0294, -0.0374,  0.2437,  0.4990, -1.0020,  0.2483,  0.6216,\n",
            "          -0.1111, -1.3117, -0.1818,  0.1142, -0.1578,  0.4042, -0.3948,\n",
            "           0.0168, -0.0363,  0.5044],\n",
            "         [-0.3796,  0.1623, -0.0717, -0.1525, -0.3240,  0.0954, -0.1359,\n",
            "          -0.2084,  0.4655,  0.0093,  0.3175,  0.3020, -1.2308,  0.3521,\n",
            "          -0.3938, -0.2990,  0.1898, -0.2936, -0.1066, -0.5964,  0.0977,\n",
            "           0.1579,  1.9672,  0.1797],\n",
            "         [-0.3947, -0.4365,  0.4851, -0.7315,  0.3451,  0.0649, -0.5335,\n",
            "           0.0936, -0.1204, -0.3624,  0.0912, -0.7716, -0.7230, -0.1404,\n",
            "          -0.8607, -0.5667, -0.3084, -0.7475, -1.2681, -0.6622, -0.1961,\n",
            "          -0.3370,  0.5396,  1.2443],\n",
            "         [ 0.5395,  0.3367, -0.0774, -0.9070,  0.2655, -1.1260, -0.8917,\n",
            "          -0.3419,  0.5167,  0.3286, -0.3794,  0.3534, -0.8603,  0.2863,\n",
            "          -0.5516, -0.5461, -1.0065, -0.7402, -0.7518, -0.0921, -0.2097,\n",
            "          -0.0695,  0.8083, -0.6859]]], device='cuda:0',\n",
            "       grad_fn=<ViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "n_layer, block_size, num_experts, top_k =  8, 32, 8, 2\n",
        "\n",
        "\n",
        "# Initialize the model\n",
        "model = VisionMoELanguageModel(n_embd, image_embed_dim, vocab_size,  n_layer, img_size, patch_size, n_head, num_blks, dropout, dropout, num_experts, top_k)\n",
        "model.to(device)\n",
        "\n",
        "# Create dummy data with correct dimensions\n",
        "dummy_img = torch.randn(1, 3, img_size, img_size).to(device)  # Correct shape for image input\n",
        "dummy_idx = torch.randint(0, vocab_size, (1, block_size)).to(device)  # Correct shape for text input\n",
        "\n",
        "# Forward pass to initialize all parameters\n",
        "try:\n",
        "    output = model(dummy_img, dummy_idx)  # Output for debugging\n",
        "    print(\"Output from initialization forward pass:\", output)\n",
        "except RuntimeError as e:\n",
        "    print(f\"Runtime Error during forward pass: {str(e)}\")\n",
        "    print(\"Check layer configurations and input shapes.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "714a426e-0732-45d3-b4d2-1e8d241e4d84",
          "showTitle": false,
          "title": ""
        },
        "id": "2Z7izIhGkJDy"
      },
      "source": [
        "Function to convert base64 encoded stringified images in inputs.csv in the repo to pytorch tensors so they can be used for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "09d59b56-a8e7-4ff4-8f0f-c0edd527f47d",
          "showTitle": false,
          "title": ""
        },
        "id": "mhoM3Rt2Rw5L"
      },
      "outputs": [],
      "source": [
        "def base64_to_tensor(base64_str, img_size=96):\n",
        "    image = Image.open(io.BytesIO(base64.b64decode(base64_str)))\n",
        "    if image.mode != 'RGB':\n",
        "        image = image.convert('RGB')\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((img_size, img_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    return transform(image).unsqueeze(0)  # Add batch dimension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "45e77024-1b6f-4b25-a946-a808305f42b8",
          "showTitle": false,
          "title": ""
        },
        "id": "qF34w9-ZRw5L"
      },
      "outputs": [],
      "source": [
        "#Adjusting the data loader from makemore for multimodal data\n",
        "def get_batch(df, batch_size, split='train', img_size=96, val_batch_size=8):\n",
        "    # Split data into training and validation sets\n",
        "    n = int(0.9 * len(df))  # first 90% will be train, rest val\n",
        "    df_train = df.iloc[:n]\n",
        "    df_val = df.iloc[n:]\n",
        "    data = df_train if split == 'train' else df_val\n",
        "    batch_size = batch_size if split == 'train' else val_batch_size\n",
        "    replace = False if split == 'train' else True\n",
        "    batch = data.sample(n=batch_size, replace=replace)\n",
        "\n",
        "    images = torch.cat([base64_to_tensor(img, img_size) for img in batch['b64string_images']], dim=0).to(device)\n",
        "    text_indices = [torch.tensor(encode(desc), dtype=torch.long) for desc in batch['caption']]\n",
        "    max_length = max(len(t) for t in text_indices)\n",
        "\n",
        "    padded_text = torch.full((batch_size, max_length), fill_value=stoi['<pad>'], dtype=torch.long).to(device)\n",
        "    for i, text in enumerate(text_indices):\n",
        "        padded_text[i, :len(text)] = text\n",
        "\n",
        "    targets = torch.cat([padded_text[:, 1:], torch.full((batch_size, 1), fill_value=stoi['<pad>'], dtype=torch.long, device=device)], dim=1)\n",
        "\n",
        "    # Truncate or pad targets to match the length of padded_text\n",
        "    if targets.size(1) > padded_text.size(1):\n",
        "        targets = targets[:, :padded_text.size(1)]\n",
        "    elif targets.size(1) < padded_text.size(1):\n",
        "        targets = torch.cat([targets, torch.full((batch_size, padded_text.size(1) - targets.size(1)), fill_value=stoi['<pad>'], dtype=torch.long, device=device)], dim=1)\n",
        "\n",
        "    return images, padded_text, targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "605036de-0c47-4dc8-9328-1a8ad9c889f8",
          "showTitle": false,
          "title": ""
        },
        "id": "IzBrV3ZWRw5L"
      },
      "outputs": [],
      "source": [
        "#Adjusting the training loop from makemore for multimodal data\n",
        "def train_model(model, df, epochs, vocab_size, img_size=96):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    model.to(device)\n",
        "    with mlflow.start_run():\n",
        "        for epoch in range(epochs):\n",
        "            model.train()\n",
        "            for _ in range(max_iters):\n",
        "                images, idx, targets = get_batch(df, batch_size, 'train', img_size)\n",
        "                optimizer.zero_grad()\n",
        "                logits, loss = model(images, idx, targets)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                if _ % eval_interval == 0:\n",
        "                    #print(f\"Loss at iteration {_}: {loss.item()}\")\n",
        "                    metrics = {\"train_loss\": loss.item()}\n",
        "                    mlflow.log_metrics(metrics, step=_)\n",
        "            val_loss = estimate_loss(model, df, 'val', img_size, val_batch_size=8)\n",
        "            metrics = {\"val_loss\": val_loss}\n",
        "            mlflow.log_metrics(metrics, step=epoch)\n",
        "            #print(f\"Validation Loss after epoch {epoch}: {val_loss}\")\n",
        "\n",
        "def estimate_loss(model, df, split, img_size=96, val_batch_size=8):\n",
        "    losses = []\n",
        "    model.eval()\n",
        "    for _ in range(eval_iters):\n",
        "        images, idx, targets = get_batch(df, batch_size, split, img_size, val_batch_size=val_batch_size)\n",
        "        _, loss = model(images, idx, targets)\n",
        "        losses.append(loss.item())\n",
        "    return sum(losses) / len(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "1670d54d-791d-4d03-945a-12aa634af044",
          "showTitle": false,
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "id": "ITWGvRmkRw5L",
        "outputId": "7e86e441-523e-41e0-8316-addb7512dcfb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📤 Please upload an image file (e.g., PNG, JPEG):\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4a8c1795-030f-404b-a48b-a6b6f1ea99cc\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-4a8c1795-030f-404b-a48b-a6b6f1ea99cc\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving starship.png to starship (3).png\n",
            "✅ Saved CSV: /content/inputs.csv with shape (300, 2)\n",
            "✅ Saved text file: /content/input.txt\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Upload an image\n",
        "print(\"📤 Please upload an image file (e.g., PNG, JPEG):\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Step 2: Get uploaded image file path\n",
        "if not uploaded:\n",
        "    raise ValueError(\"❌ No image uploaded.\")\n",
        "image_filename = next(iter(uploaded))\n",
        "\n",
        "# Step 3: Convert image to base64\n",
        "def image_to_base64(image_bytes, img_size=96, format=\"PNG\"):\n",
        "    \"\"\"Resize image to img_size x img_size and return base64 string.\"\"\"\n",
        "    image = Image.open(image_bytes).convert(\"RGB\")\n",
        "    image = image.resize((img_size, img_size), Image.LANCZOS)\n",
        "    buffer = io.BytesIO()\n",
        "    image.save(buffer, format=format)\n",
        "    return base64.b64encode(buffer.getvalue()).decode('utf-8')\n",
        "\n",
        "img_base64 = image_to_base64(io.BytesIO(uploaded[image_filename]))\n",
        "\n",
        "# Step 4: Create a DataFrame with repeated base64 images + captions\n",
        "num_rows = 10\n",
        "df = pd.DataFrame({\n",
        "    'b64string_images': [img_base64] * num_rows,\n",
        "    'caption': [f\"Sample caption {i+1} for testing\" for i in range(num_rows)]\n",
        "})\n",
        "\n",
        "# Step 5: Expand to simulate larger dataset\n",
        "df = pd.concat([df] * 30, ignore_index=True)\n",
        "\n",
        "# Step 6: Save to CSV\n",
        "csv_path = \"/content/inputs.csv\"\n",
        "df.to_csv(csv_path, index=False)\n",
        "print(f\"✅ Saved CSV: {csv_path} with shape {df.shape}\")\n",
        "\n",
        "# Step 7: Create sample input.txt for tokenizer tests\n",
        "text = \"This is a sample text for testing the model.\\nHello world!\"\n",
        "text_path = \"/content/input.txt\"\n",
        "with open(text_path, 'w', encoding='utf-8') as f:\n",
        "    f.write(text)\n",
        "print(f\"✅ Saved text file: {text_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWt2VtkcoaDs",
        "outputId": "d05e0fd9-1d9b-4b7a-cf8d-3b2ac3b85fb1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(300, 2)"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv(\"./inputs.csv\")\n",
        "#Expanding dataframe so that there's enough data to test. This is just duplicating data. A real dataset would have more rows\n",
        "df = pd.concat([df])[['b64string_images', 'caption']]\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "a91f0e87-d361-40a3-a43f-51566e03810a",
          "showTitle": false,
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "QgyBgOPJRw5L",
        "outputId": "744f3f0e-34dd-4585-9b72-eaab55df4503"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"display(df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"b64string_images\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"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\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"caption\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Sample caption 2 for testing\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-7ee9349c-2c16-4055-8179-3484a1415d2d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>b64string_images</th>\n",
              "      <th>caption</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>iVBORw0KGgoAAAANSUhEUgAAAGAAAABgCAIAAABt+uBvAA...</td>\n",
              "      <td>Sample caption 1 for testing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>iVBORw0KGgoAAAANSUhEUgAAAGAAAABgCAIAAABt+uBvAA...</td>\n",
              "      <td>Sample caption 2 for testing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>iVBORw0KGgoAAAANSUhEUgAAAGAAAABgCAIAAABt+uBvAA...</td>\n",
              "      <td>Sample caption 3 for testing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>iVBORw0KGgoAAAANSUhEUgAAAGAAAABgCAIAAABt+uBvAA...</td>\n",
              "      <td>Sample caption 4 for testing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>iVBORw0KGgoAAAANSUhEUgAAAGAAAABgCAIAAABt+uBvAA...</td>\n",
              "      <td>Sample caption 5 for testing</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7ee9349c-2c16-4055-8179-3484a1415d2d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7ee9349c-2c16-4055-8179-3484a1415d2d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7ee9349c-2c16-4055-8179-3484a1415d2d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-ba934c36-0cd1-478d-a447-e64457a30028\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ba934c36-0cd1-478d-a447-e64457a30028')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-ba934c36-0cd1-478d-a447-e64457a30028 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                    b64string_images  \\\n",
              "0  iVBORw0KGgoAAAANSUhEUgAAAGAAAABgCAIAAABt+uBvAA...   \n",
              "1  iVBORw0KGgoAAAANSUhEUgAAAGAAAABgCAIAAABt+uBvAA...   \n",
              "2  iVBORw0KGgoAAAANSUhEUgAAAGAAAABgCAIAAABt+uBvAA...   \n",
              "3  iVBORw0KGgoAAAANSUhEUgAAAGAAAABgCAIAAABt+uBvAA...   \n",
              "4  iVBORw0KGgoAAAANSUhEUgAAAGAAAABgCAIAAABt+uBvAA...   \n",
              "\n",
              "                        caption  \n",
              "0  Sample caption 1 for testing  \n",
              "1  Sample caption 2 for testing  \n",
              "2  Sample caption 3 for testing  \n",
              "3  Sample caption 4 for testing  \n",
              "4  Sample caption 5 for testing  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "3d948df0-4979-46e1-8208-629e8a95c93b",
          "showTitle": false,
          "title": ""
        },
        "id": "MFM46Cs0Rw5L"
      },
      "outputs": [],
      "source": [
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 100\n",
        "eval_interval = 10\n",
        "learning_rate = 1e-3\n",
        "epochs=2 # Increased epochs\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 40\n",
        "num_blks= 3\n",
        "head_size = 16\n",
        "n_embd = 128\n",
        "n_head = 8\n",
        "n_layer = 8\n",
        "dropout = 0.1\n",
        "img_size=96\n",
        "patch_size =16\n",
        "image_embed_dim = 512\n",
        "emb_dropout = blk_dropout =0.1\n",
        "num_experts, top_k = 8, 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "60961b04-d9db-4540-a9d5-5c6f57a0d68b",
          "showTitle": false,
          "title": ""
        },
        "id": "906xj4cWkDk0"
      },
      "outputs": [],
      "source": [
        "def kaiming_init_weights(m):\n",
        "    if isinstance (m, (nn.Linear)):\n",
        "        init.kaiming_normal_(m.weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "04e27881-aaee-4f40-b5cd-4e6a2de0fca7",
          "showTitle": false,
          "title": ""
        },
        "id": "ayLj98T7Rw5L"
      },
      "outputs": [],
      "source": [
        "# Initialize the model\n",
        "model = VisionMoELanguageModel(n_embd, image_embed_dim, vocab_size, n_layer, img_size, patch_size, n_head, num_blks, emb_dropout, blk_dropout, num_experts, top_k)\n",
        "#Appluy thre initialization\n",
        "model.apply(kaiming_init_weights)\n",
        "model.to(device)\n",
        "\n",
        "# Dummy data to initialize lazy modules\n",
        "dummy_img = torch.randn(1, 3, img_size, img_size).to(device)\n",
        "dummy_idx = torch.randint(0, vocab_size, (1, block_size)).to(device)\n",
        "model(dummy_img, dummy_idx)  # Forward pass to initialize all parameters\n",
        "\n",
        "# Train the model\n",
        "train_model(model, df, epochs, vocab_size, img_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8319caf5",
        "outputId": "69d89b19-01fc-405c-a2e2-5b5b046819ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated text: Samplee<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
          ]
        }
      ],
      "source": [
        "# Test generation\n",
        "model.eval()\n",
        "test_img = base64_to_tensor(df['b64string_images'].iloc[0], img_size).to(device)\n",
        "test_idx = torch.tensor([encode(\"Sample\")], dtype=torch.long).to(device)\n",
        "generated = model.generate(test_img, test_idx, max_new_tokens=20)\n",
        "print(f\"Generated text: {decode(generated[0].tolist())}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0f6549b"
      },
      "outputs": [],
      "source": [
        "# Initialize the model\n",
        "model = VisionMoELanguageModel(n_embd, image_embed_dim, vocab_size, n_layer, img_size, patch_size, n_head, num_blks, emb_dropout, blk_dropout, num_experts, top_k)\n",
        "#Appluy thre initialization\n",
        "model.apply(kaiming_init_weights)\n",
        "model.to(device)\n",
        "\n",
        "# Dummy data to initialize lazy modules\n",
        "dummy_img = torch.randn(1, 3, img_size, img_size).to(device)\n",
        "dummy_idx = torch.randint(0, vocab_size, (1, block_size)).to(device)\n",
        "model(dummy_img, dummy_idx)  # Forward pass to initialize all parameters\n",
        "\n",
        "# Train the model\n",
        "train_model(model, df, epochs, vocab_size, img_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9348bffb"
      },
      "outputs": [],
      "source": [
        "# Initialize the model\n",
        "model = VisionMoELanguageModel(n_embd, image_embed_dim, vocab_size, n_layer, img_size, patch_size, n_head, num_blks, emb_dropout, blk_dropout, num_experts, top_k)\n",
        "#Appluy thre initialization\n",
        "model.apply(kaiming_init_weights)\n",
        "model.to(device)\n",
        "\n",
        "# Dummy data to initialize lazy modules\n",
        "dummy_img = torch.randn(1, 3, img_size, img_size).to(device)\n",
        "dummy_idx = torch.randint(0, vocab_size, (1, block_size)).to(device)\n",
        "model(dummy_img, dummy_idx)  # Forward pass to initialize all parameters\n",
        "\n",
        "# Train the model\n",
        "train_model(model, df, epochs, vocab_size, img_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfcaa23b",
        "outputId": "ff83cfbf-6005-4d4e-d998-c1eaa6849993"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated text: A photo oftn <pad>ng co<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad> t<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>g<pad><pad><pad>\n"
          ]
        }
      ],
      "source": [
        "# Test generation using the uploaded image\n",
        "model.eval()\n",
        "# Assuming the uploaded image is stored in the 'uploaded' variable from the previous upload step\n",
        "image_filename = next(iter(uploaded))\n",
        "test_img_bytes = io.BytesIO(uploaded[image_filename])\n",
        "test_img = base64_to_tensor(image_to_base64(test_img_bytes, img_size), img_size).to(device)\n",
        "\n",
        "# Start generation with a prompt\n",
        "prompt = \"A photo of\"\n",
        "test_idx = torch.tensor([encode(prompt)], dtype=torch.long).to(device)\n",
        "\n",
        "generated = model.generate(test_img, test_idx, max_new_tokens=50)\n",
        "print(f\"Generated text: {decode(generated[0].tolist())}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f587ef24"
      },
      "outputs": [],
      "source": [
        "!pip install wandb -qq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "d3c06112",
        "outputId": "37b9acdc-df48-4c97-e1e2-b5a889abf017"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mderrickkirimi-dev\u001b[0m (\u001b[33mderrickkirimi-dev-qnt\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import wandb\n",
        "\n",
        "# Log in to your wandb account\n",
        "# You will be prompted to enter your API key\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "830dde5c"
      },
      "outputs": [],
      "source": [
        "#Adjusting the training loop from makemore for multimodal data\n",
        "def train_model(model, df, epochs, vocab_size, img_size=96):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for iter in range(max_iters):\n",
        "            images, idx, targets = get_batch(df, batch_size, 'train', img_size)\n",
        "            optimizer.zero_grad()\n",
        "            logits, loss = model(images, idx, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if iter % eval_interval == 0:\n",
        "                train_loss = loss.item()\n",
        "                val_loss = estimate_loss(model, df, 'val', img_size, val_batch_size=8)\n",
        "                print(f\"Epoch {epoch+1}, Iter {iter}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "                # Log metrics to wandb\n",
        "                wandb.log({\"train_loss\": train_loss, \"val_loss\": val_loss}, step=epoch * max_iters + iter)\n",
        "\n",
        "        # Save model checkpoint at the end of each epoch\n",
        "        torch.save(model.state_dict(), f\"model_epoch_{epoch+1}.pt\")\n",
        "        wandb.save(f\"model_epoch_{epoch+1}.pt\")\n",
        "        print(f\"Checkpoint saved for epoch {epoch+1}\")\n",
        "\n",
        "def estimate_loss(model, df, split, img_size=96, val_batch_size=8):\n",
        "    losses = []\n",
        "    model.eval()\n",
        "    with torch.no_grad(): # Use no_grad for evaluation\n",
        "        for _ in range(eval_iters):\n",
        "            images, idx, targets = get_batch(df, batch_size, split, img_size, val_batch_size=val_batch_size)\n",
        "            _, loss = model(images, idx, targets)\n",
        "            losses.append(loss.item())\n",
        "    return sum(losses) / len(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "17aa2987",
        "outputId": "f02000d0-2fe6-4fa5-ce9d-414a3fd881e6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250626_161501-i7tzb2p2</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/derrickkirimi-dev-qnt/vision-moe-multimodal/runs/i7tzb2p2' target=\"_blank\">fallen-sound-1</a></strong> to <a href='https://wandb.ai/derrickkirimi-dev-qnt/vision-moe-multimodal' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/derrickkirimi-dev-qnt/vision-moe-multimodal' target=\"_blank\">https://wandb.ai/derrickkirimi-dev-qnt/vision-moe-multimodal</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/derrickkirimi-dev-qnt/vision-moe-multimodal/runs/i7tzb2p2' target=\"_blank\">https://wandb.ai/derrickkirimi-dev-qnt/vision-moe-multimodal/runs/i7tzb2p2</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Iter 0: Train Loss: 5.2938, Val Loss: 4.0191\n",
            "Epoch 1, Iter 10: Train Loss: 2.8940, Val Loss: 2.8745\n",
            "Epoch 1, Iter 20: Train Loss: 2.6650, Val Loss: 2.6057\n",
            "Epoch 1, Iter 30: Train Loss: 2.3271, Val Loss: 2.3446\n",
            "Epoch 1, Iter 40: Train Loss: 2.0101, Val Loss: 1.9485\n",
            "Epoch 1, Iter 50: Train Loss: 1.3745, Val Loss: 1.2710\n",
            "Epoch 1, Iter 60: Train Loss: 0.4640, Val Loss: 0.3497\n",
            "Epoch 1, Iter 70: Train Loss: 0.4720, Val Loss: 0.2646\n",
            "Epoch 1, Iter 80: Train Loss: 0.1822, Val Loss: 0.1932\n",
            "Epoch 1, Iter 90: Train Loss: 0.1150, Val Loss: 0.1412\n",
            "Checkpoint saved for epoch 1\n",
            "Epoch 2, Iter 0: Train Loss: 0.5305, Val Loss: 0.0967\n",
            "Epoch 2, Iter 10: Train Loss: 0.1098, Val Loss: 0.0984\n",
            "Epoch 2, Iter 20: Train Loss: 0.0855, Val Loss: 0.0914\n",
            "Epoch 2, Iter 30: Train Loss: 0.0867, Val Loss: 0.0878\n",
            "Epoch 2, Iter 40: Train Loss: 0.0797, Val Loss: 0.0918\n",
            "Epoch 2, Iter 50: Train Loss: 0.0908, Val Loss: 0.0914\n",
            "Epoch 2, Iter 60: Train Loss: 0.0853, Val Loss: 0.0851\n",
            "Epoch 2, Iter 70: Train Loss: 0.0840, Val Loss: 0.0840\n",
            "Epoch 2, Iter 80: Train Loss: 0.0819, Val Loss: 0.0879\n",
            "Epoch 2, Iter 90: Train Loss: 0.0854, Val Loss: 0.0840\n",
            "Checkpoint saved for epoch 2\n",
            "Epoch 3, Iter 0: Train Loss: 0.1553, Val Loss: 0.0817\n",
            "Epoch 3, Iter 10: Train Loss: 0.0860, Val Loss: 0.0852\n",
            "Epoch 3, Iter 20: Train Loss: 0.0815, Val Loss: 0.0851\n",
            "Epoch 3, Iter 30: Train Loss: 0.0768, Val Loss: 0.0829\n",
            "Epoch 3, Iter 40: Train Loss: 0.0826, Val Loss: 0.0846\n",
            "Epoch 3, Iter 50: Train Loss: 0.0841, Val Loss: 0.0827\n",
            "Epoch 3, Iter 60: Train Loss: 0.0793, Val Loss: 0.0845\n",
            "Epoch 3, Iter 70: Train Loss: 0.0800, Val Loss: 0.0839\n",
            "Epoch 3, Iter 80: Train Loss: 0.0884, Val Loss: 0.0858\n",
            "Epoch 3, Iter 90: Train Loss: 0.0899, Val Loss: 0.0845\n",
            "Checkpoint saved for epoch 3\n",
            "Epoch 4, Iter 0: Train Loss: 0.1168, Val Loss: 0.0842\n",
            "Epoch 4, Iter 10: Train Loss: 0.0829, Val Loss: 0.0874\n",
            "Epoch 4, Iter 20: Train Loss: 0.0918, Val Loss: 0.0879\n",
            "Epoch 4, Iter 30: Train Loss: 0.2222, Val Loss: 0.1028\n",
            "Epoch 4, Iter 40: Train Loss: 0.1556, Val Loss: 0.1015\n",
            "Epoch 4, Iter 50: Train Loss: 0.0897, Val Loss: 0.0962\n",
            "Epoch 4, Iter 60: Train Loss: 0.0844, Val Loss: 0.0862\n",
            "Epoch 4, Iter 70: Train Loss: 0.1205, Val Loss: 0.1206\n",
            "Epoch 4, Iter 80: Train Loss: 0.1034, Val Loss: 0.1260\n",
            "Epoch 4, Iter 90: Train Loss: 0.1554, Val Loss: 0.2166\n",
            "Checkpoint saved for epoch 4\n",
            "Epoch 5, Iter 0: Train Loss: 0.8560, Val Loss: 0.5377\n",
            "Epoch 5, Iter 10: Train Loss: 0.1613, Val Loss: 0.1526\n",
            "Epoch 5, Iter 20: Train Loss: 0.1020, Val Loss: 0.1305\n",
            "Epoch 5, Iter 30: Train Loss: 0.9026, Val Loss: 0.5024\n",
            "Epoch 5, Iter 40: Train Loss: 1.0597, Val Loss: 0.8418\n",
            "Epoch 5, Iter 50: Train Loss: 0.2957, Val Loss: 0.3006\n",
            "Epoch 5, Iter 60: Train Loss: 0.1222, Val Loss: 0.1308\n",
            "Epoch 5, Iter 70: Train Loss: 0.0928, Val Loss: 0.0953\n",
            "Epoch 5, Iter 80: Train Loss: 0.1333, Val Loss: 0.0952\n",
            "Epoch 5, Iter 90: Train Loss: 0.0798, Val Loss: 0.0860\n",
            "Checkpoint saved for epoch 5\n",
            "Epoch 6, Iter 0: Train Loss: 0.2747, Val Loss: 0.0871\n",
            "Epoch 6, Iter 10: Train Loss: 0.0839, Val Loss: 0.0842\n",
            "Epoch 6, Iter 20: Train Loss: 0.1559, Val Loss: 0.2049\n",
            "Epoch 6, Iter 30: Train Loss: 0.1037, Val Loss: 0.1087\n",
            "Epoch 6, Iter 40: Train Loss: 0.0850, Val Loss: 0.1112\n",
            "Epoch 6, Iter 50: Train Loss: 0.1115, Val Loss: 0.0910\n",
            "Epoch 6, Iter 60: Train Loss: 0.0833, Val Loss: 0.0853\n",
            "Epoch 6, Iter 70: Train Loss: 0.0806, Val Loss: 0.0855\n",
            "Epoch 6, Iter 80: Train Loss: 0.0830, Val Loss: 0.0823\n",
            "Epoch 6, Iter 90: Train Loss: 0.0822, Val Loss: 0.0848\n",
            "Checkpoint saved for epoch 6\n",
            "Epoch 7, Iter 0: Train Loss: 0.1783, Val Loss: 0.0862\n",
            "Epoch 7, Iter 10: Train Loss: 0.1370, Val Loss: 0.1404\n",
            "Epoch 7, Iter 20: Train Loss: 0.1784, Val Loss: 0.1590\n",
            "Epoch 7, Iter 30: Train Loss: 0.1280, Val Loss: 0.1320\n",
            "Epoch 7, Iter 40: Train Loss: 0.1242, Val Loss: 0.1210\n",
            "Epoch 7, Iter 50: Train Loss: 0.1141, Val Loss: 0.0992\n",
            "Epoch 7, Iter 60: Train Loss: 0.0846, Val Loss: 0.0850\n",
            "Epoch 7, Iter 70: Train Loss: 0.0852, Val Loss: 0.0836\n",
            "Epoch 7, Iter 80: Train Loss: 0.0809, Val Loss: 0.0828\n",
            "Epoch 7, Iter 90: Train Loss: 0.0847, Val Loss: 0.0836\n",
            "Checkpoint saved for epoch 7\n",
            "Epoch 8, Iter 0: Train Loss: 0.1554, Val Loss: 0.0822\n",
            "Epoch 8, Iter 10: Train Loss: 0.0807, Val Loss: 0.0843\n",
            "Epoch 8, Iter 20: Train Loss: 0.0843, Val Loss: 0.0834\n",
            "Epoch 8, Iter 30: Train Loss: 0.0855, Val Loss: 0.0928\n",
            "Epoch 8, Iter 40: Train Loss: 0.0804, Val Loss: 0.0852\n",
            "Epoch 8, Iter 50: Train Loss: 0.0819, Val Loss: 0.0827\n",
            "Epoch 8, Iter 60: Train Loss: 0.0801, Val Loss: 0.0826\n",
            "Epoch 8, Iter 70: Train Loss: 0.0842, Val Loss: 0.0838\n",
            "Epoch 8, Iter 80: Train Loss: 0.0826, Val Loss: 0.0839\n",
            "Epoch 8, Iter 90: Train Loss: 0.0858, Val Loss: 0.0823\n",
            "Checkpoint saved for epoch 8\n",
            "Epoch 9, Iter 0: Train Loss: 0.1069, Val Loss: 0.0815\n",
            "Epoch 9, Iter 10: Train Loss: 0.0843, Val Loss: 0.0852\n",
            "Epoch 9, Iter 20: Train Loss: 0.0777, Val Loss: 0.0823\n",
            "Epoch 9, Iter 30: Train Loss: 0.0792, Val Loss: 0.0834\n",
            "Epoch 9, Iter 40: Train Loss: 0.0811, Val Loss: 0.0839\n",
            "Epoch 9, Iter 50: Train Loss: 0.0828, Val Loss: 0.0826\n",
            "Epoch 9, Iter 60: Train Loss: 0.0800, Val Loss: 0.0826\n",
            "Epoch 9, Iter 70: Train Loss: 0.0799, Val Loss: 0.0831\n",
            "Epoch 9, Iter 80: Train Loss: 0.0806, Val Loss: 0.0820\n",
            "Epoch 9, Iter 90: Train Loss: 0.0807, Val Loss: 0.0820\n",
            "Checkpoint saved for epoch 9\n",
            "Epoch 10, Iter 0: Train Loss: 0.1151, Val Loss: 0.0818\n",
            "Epoch 10, Iter 10: Train Loss: 0.2069, Val Loss: 0.0884\n",
            "Epoch 10, Iter 20: Train Loss: 0.0823, Val Loss: 0.0998\n",
            "Epoch 10, Iter 30: Train Loss: 0.0888, Val Loss: 0.0822\n",
            "Epoch 10, Iter 40: Train Loss: 0.0813, Val Loss: 0.0849\n",
            "Epoch 10, Iter 50: Train Loss: 0.0790, Val Loss: 0.0847\n",
            "Epoch 10, Iter 60: Train Loss: 0.0826, Val Loss: 0.0843\n",
            "Epoch 10, Iter 70: Train Loss: 0.0801, Val Loss: 0.0823\n",
            "Epoch 10, Iter 80: Train Loss: 0.0894, Val Loss: 0.0841\n",
            "Epoch 10, Iter 90: Train Loss: 0.0780, Val Loss: 0.0863\n",
            "Checkpoint saved for epoch 10\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td>█▄▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td>0.07802</td></tr><tr><td>val_loss</td><td>0.08634</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fallen-sound-1</strong> at: <a href='https://wandb.ai/derrickkirimi-dev-qnt/vision-moe-multimodal/runs/i7tzb2p2' target=\"_blank\">https://wandb.ai/derrickkirimi-dev-qnt/vision-moe-multimodal/runs/i7tzb2p2</a><br> View project at: <a href='https://wandb.ai/derrickkirimi-dev-qnt/vision-moe-multimodal' target=\"_blank\">https://wandb.ai/derrickkirimi-dev-qnt/vision-moe-multimodal</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250626_161501-i7tzb2p2/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Initialize a wandb run outside the training loop\n",
        "run = wandb.init(project=\"vision-moe-multimodal\", config={\n",
        "    \"epochs\": epochs,\n",
        "    \"batch_size\": batch_size,\n",
        "    \"block_size\": block_size,\n",
        "    \"learning_rate\": learning_rate,\n",
        "    \"eval_interval\": eval_interval,\n",
        "    \"eval_iters\": eval_iters,\n",
        "    \"n_embd\": n_embd,\n",
        "    \"n_head\": n_head,\n",
        "    \"n_layer\": n_layer,\n",
        "    \"dropout\": dropout,\n",
        "    \"img_size\": img_size,\n",
        "    \"patch_size\": patch_size,\n",
        "    \"image_embed_dim\": image_embed_dim,\n",
        "    \"emb_dropout\": emb_dropout,\n",
        "    \"blk_dropout\": blk_dropout,\n",
        "    \"num_experts\": num_experts,\n",
        "    \"top_k\": top_k,\n",
        "    \"num_blks\": num_blks,\n",
        "    \"vocab_size\": vocab_size,\n",
        "})\n",
        "\n",
        "# Initialize and train the model\n",
        "model = VisionMoELanguageModel(n_embd, image_embed_dim, vocab_size, n_layer, img_size, patch_size, n_head, num_blks, emb_dropout, blk_dropout, num_experts, top_k)\n",
        "model.apply(kaiming_init_weights)\n",
        "model.to(device)\n",
        "train_model(model, df, epochs, vocab_size, img_size)\n",
        "\n",
        "# Finish the wandb run\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d719a1ce"
      },
      "source": [
        "\n",
        "## Upload the trained PyTorch model to Hugging Face Hub."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98180f86"
      },
      "source": [
        "## Save the model\n",
        "\n",
        "Save the trained PyTorch model state dictionary.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "2cf822d4"
      },
      "outputs": [],
      "source": [
        "model_save_path = \"vision_moe_model.pth\"\n",
        "torch.save(model.state_dict(), model_save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "fc8edf8f"
      },
      "outputs": [],
      "source": [
        "!pip install transformers huggingface_hub -qq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5c261298"
      },
      "source": [
        "## Log in to hugging face\n",
        "\n",
        "Authenticate with your Hugging Face account using the `huggingface_hub` library.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "7717ff13f655462990ca268d59367f43",
            "9cdd419f6aa04283a228b2272d96187c",
            "7ac3628f12e1448da274cfde07bd52cf",
            "89bf6b9af07d4326a6ca2bcf772616be",
            "f3f3fe702681452c9c2690793869599c",
            "4475ee0e23cd4fec8f2b252d0b74f305",
            "dbb8d7e566734f27a86df131b203d04a",
            "a7dbc42188bd4a6d86f53a0c62ff6d2d",
            "16cf4a7400534312b16df4950c890343",
            "c0405bd4597d4296844d3102846130a0",
            "8bed13faed5644f7948aa0168bd7bae2",
            "5fabc06325dc4c4496e66e4bf960c149",
            "bf9f9a46f7c54c1b81fd7ab6930f2835",
            "e6d2423658b44f468c4d0d45ba128a32",
            "fa483215fe44434ab013455eaa1b7b71",
            "67c85fa68e654bf19f82a688b1ff7b25",
            "5ec1baffb2ad4863b570276369d47c5c",
            "bb29b1a96e404ce69f302964bea2619c",
            "6392cb308cee4eb4887270feef218948",
            "8ce2a819b3384406ab8666df3352267b"
          ]
        },
        "id": "8db90b2c",
        "outputId": "62853d89-e98f-4462-957d-3c5837527d6f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7717ff13f655462990ca268d59367f43",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eef6c24a",
        "outputId": "220a2eec-9716-45ef-e66b-c0fa58d926f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Repository 'Aptheos/SparseFusion' created or confirmed on Hugging Face Hub.\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import HfApi\n",
        "\n",
        "api = HfApi()\n",
        "repo_id = \"Aptheos/SparseFusion\"\n",
        "api.create_repo(repo_id=repo_id, repo_type=\"model\", exist_ok=True)\n",
        "print(f\"Repository '{repo_id}' created or confirmed on Hugging Face Hub.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "1f94689e36614518bd1cf206360d112a",
            "3649683856124052afb939b3fdea8a11",
            "11d9f3c77c7e45008894df110421237e",
            "06db7015b60d4f8292fed03fd7af5c1d",
            "8c263d0764f5485c8749100d3ef8ad08",
            "05cba355825c4334adb21a0af4abf42a",
            "fc8ac87f059c44a1a160aba71de00dde",
            "a188d6ed27404fde8a391c0f0859b953",
            "7b6eb948f0854cbf96a64e318962e6a5",
            "951ec894ecd544fc8595134ce1ea74b7",
            "ee0a8ea7110e492db3aaa4a1aef308be"
          ]
        },
        "id": "27253790",
        "outputId": "723b1bbc-b142-4dbc-a3b0-d414ab350805"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1f94689e36614518bd1cf206360d112a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Uploading...:   0%|          | 0.00/81.5M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully uploaded vision_moe_model.pth to Aptheos/SparseFusion\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import HfApi, upload_file\n",
        "\n",
        "api = HfApi()\n",
        "repo_id = \"Aptheos/SparseFusion\"\n",
        "\n",
        "# Upload the model state dictionary file\n",
        "model_path = \"vision_moe_model.pth\"\n",
        "upload_file(\n",
        "    path_or_fileobj=model_path,\n",
        "    path_in_repo=model_path,\n",
        "    repo_id=repo_id,\n",
        "    repo_type=\"model\",\n",
        ")\n",
        "\n",
        "print(f\"Successfully uploaded {model_path} to {repo_id}\")\n",
        "\n",
        "# There might be need to upload vocabulary files if they are not part of the model state dict\n",
        "# For example, if stoi and itos are saved:\n",
        "# upload_file(\n",
        "#     path_or_fileobj=\"stoi.pkl\", # If stoi is saved as a pickle file\n",
        "#     path_in_repo=\"stoi.pkl\",\n",
        "#     repo_id=repo_id,\n",
        "#     repo_type=\"model\",\n",
        "# )\n",
        "# upload_file(\n",
        "#     path_or_fileobj=\"itos.pkl\",\n",
        "#     path_in_repo=\"itos.pkl\",\n",
        "#     repo_id=repo_id,\n",
        "#     repo_type=\"model\",\n",
        "# )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "application/vnd.databricks.v1+notebook": {
      "dashboards": [],
      "language": "python",
      "notebookMetadata": {
        "pythonIndentUnit": 2
      },
      "notebookName": "seeMoE_from_Scratch",
      "widgets": {}
    },
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "05cba355825c4334adb21a0af4abf42a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06db7015b60d4f8292fed03fd7af5c1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_951ec894ecd544fc8595134ce1ea74b7",
            "placeholder": "​",
            "style": "IPY_MODEL_ee0a8ea7110e492db3aaa4a1aef308be",
            "value": " 81.5M/81.5M [00:04&lt;00:00, 49.1MB/s]"
          }
        },
        "11d9f3c77c7e45008894df110421237e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a188d6ed27404fde8a391c0f0859b953",
            "max": 81495104,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7b6eb948f0854cbf96a64e318962e6a5",
            "value": 81495104
          }
        },
        "16cf4a7400534312b16df4950c890343": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1f94689e36614518bd1cf206360d112a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3649683856124052afb939b3fdea8a11",
              "IPY_MODEL_11d9f3c77c7e45008894df110421237e",
              "IPY_MODEL_06db7015b60d4f8292fed03fd7af5c1d"
            ],
            "layout": "IPY_MODEL_8c263d0764f5485c8749100d3ef8ad08"
          }
        },
        "3649683856124052afb939b3fdea8a11": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_05cba355825c4334adb21a0af4abf42a",
            "placeholder": "​",
            "style": "IPY_MODEL_fc8ac87f059c44a1a160aba71de00dde",
            "value": "Uploading...: 100%"
          }
        },
        "4475ee0e23cd4fec8f2b252d0b74f305": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_67c85fa68e654bf19f82a688b1ff7b25",
            "placeholder": "​",
            "style": "IPY_MODEL_5ec1baffb2ad4863b570276369d47c5c",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "5ec1baffb2ad4863b570276369d47c5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5fabc06325dc4c4496e66e4bf960c149": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6392cb308cee4eb4887270feef218948": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67c85fa68e654bf19f82a688b1ff7b25": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7717ff13f655462990ca268d59367f43": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_dbb8d7e566734f27a86df131b203d04a"
          }
        },
        "7ac3628f12e1448da274cfde07bd52cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "PasswordModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_c0405bd4597d4296844d3102846130a0",
            "placeholder": "​",
            "style": "IPY_MODEL_8bed13faed5644f7948aa0168bd7bae2",
            "value": ""
          }
        },
        "7b6eb948f0854cbf96a64e318962e6a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "89bf6b9af07d4326a6ca2bcf772616be": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "CheckboxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_5fabc06325dc4c4496e66e4bf960c149",
            "style": "IPY_MODEL_bf9f9a46f7c54c1b81fd7ab6930f2835",
            "value": true
          }
        },
        "8bed13faed5644f7948aa0168bd7bae2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8c263d0764f5485c8749100d3ef8ad08": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ce2a819b3384406ab8666df3352267b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "951ec894ecd544fc8595134ce1ea74b7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9cdd419f6aa04283a228b2272d96187c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a7dbc42188bd4a6d86f53a0c62ff6d2d",
            "placeholder": "​",
            "style": "IPY_MODEL_16cf4a7400534312b16df4950c890343",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "a188d6ed27404fde8a391c0f0859b953": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7dbc42188bd4a6d86f53a0c62ff6d2d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb29b1a96e404ce69f302964bea2619c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6392cb308cee4eb4887270feef218948",
            "placeholder": "​",
            "style": "IPY_MODEL_8ce2a819b3384406ab8666df3352267b",
            "value": "Connecting..."
          }
        },
        "bf9f9a46f7c54c1b81fd7ab6930f2835": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c0405bd4597d4296844d3102846130a0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dbb8d7e566734f27a86df131b203d04a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "e6d2423658b44f468c4d0d45ba128a32": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee0a8ea7110e492db3aaa4a1aef308be": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f3f3fe702681452c9c2690793869599c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_e6d2423658b44f468c4d0d45ba128a32",
            "style": "IPY_MODEL_fa483215fe44434ab013455eaa1b7b71",
            "tooltip": ""
          }
        },
        "fa483215fe44434ab013455eaa1b7b71": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "fc8ac87f059c44a1a160aba71de00dde": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
